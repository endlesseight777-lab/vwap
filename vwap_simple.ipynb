{"cells": [{"cell_type": "code", "metadata": {}, "source": ["# ===========================================\n", "#  VWAP optimal execution \u00d7 NN \u00d7 q_T=0 constraint\n", "#  v_t \u3092 NN \u304c\u76f4\u63a5\u51fa\u529b\u3059\u308b\u7c21\u6f54\u7248\uff08\u5b9f\u9a13\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4e0a\uff09\n", "# ===========================================\n", "\n", "import math\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "import matplotlib.pyplot as plt\n", "\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "print(\"device =\", device)\n", "\n", "\n", "# ===== \u5b9f\u9a13\u30d1\u30e9\u30e1\u30fc\u30bf (\u3053\u3053\u3060\u3051\u5909\u3048\u308c\u3070OK) =====\n", "S0    = 50.0\n", "q0    = 400000.0\n", "V     = 4_000_000.0\n", "sigma = 0.45\n", "eta   = 0.12\n", "phi   = 0.63\n", "k     = 5e-7\n", "gamma = 3e-6\n", "lambda_qT = 1e-8\n", "\n", "T = 1.0\n", "J = 200\n", "num_epochs = 300\n", "batch_size = 256\n", "hidden_dim = 128\n", "\n", "\n", "# ===== NN (PolicyNet) =====\n", "class PolicyNet(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.v_max = 2.0 * q0 / T\n", "        self.net = nn.Sequential(\n", "            nn.Linear(4, hidden_dim),\n", "            nn.ReLU(),\n", "            nn.Linear(hidden_dim, hidden_dim),\n", "            nn.ReLU(),\n", "            nn.Linear(hidden_dim, 1),\n", "            nn.Tanh(),\n", "        )\n", "\n", "    def forward(self, state):\n", "        return self.v_max * self.net(state)\n", "\n", "\n", "# ===== \u30e2\u30f3\u30c6\u30ab\u30eb\u30ed (\u5b66\u7fd2\u7528) =====\n", "def simulate_batch(policy):\n", "    dt = T / J\n", "    sqrt_dt = math.sqrt(dt)\n", "    S = torch.full((batch_size,1), S0, device=device)\n", "    q = torch.full((batch_size,1), q0, device=device)\n", "    X = torch.zeros((batch_size,1), device=device)\n", "    Q = torch.zeros((batch_size,1), device=device)\n", "    vw_num = torch.zeros((batch_size,1), device=device)\n", "    vw_den = torch.zeros((batch_size,1), device=device)\n", "    eps = torch.randn(batch_size, J, device=device)\n", "    S_ref, V_ref, Q_ref = S0, V, V*T\n", "    q_ref = max(abs(q0),1.0)\n", "    policy.train()\n", "    for j in range(J):\n", "        V_t = torch.full_like(S, V)\n", "        state = torch.cat([\n", "            S/S_ref, \n", "            V_t/V_ref,\n", "            Q/Q_ref,\n", "            q/q_ref\n", "        ], dim=1)\n", "        v = policy(state)\n", "        dq = v * dt\n", "        q_next = q - dq\n", "        rho = v / V_t\n", "        L = eta * torch.abs(rho)**(1+phi)\n", "        X = X + (v*S - V_t*L)*dt\n", "        dW = eps[:,j:j+1]*sqrt_dt\n", "        S = S + sigma*dW - k*v*dt\n", "        Q = Q + V_t*dt\n", "        vw_num += S * V_t * dt\n", "        vw_den += V_t * dt\n", "        q = q_next\n", "    VWAP_T = vw_num / vw_den\n", "    pnl = X - q0 * VWAP_T\n", "    pnl = torch.clamp(pnl, -1e6, 1e6)\n", "    utility = torch.exp(-gamma * pnl)\n", "    penalty = lambda_qT * (q.squeeze(1)**2)\n", "    loss = (utility + penalty).mean()\n", "    return loss\n", "\n", "\n", "# ===== \u5b66\u7fd2 =====\n", "def train():\n", "    policy = PolicyNet().to(device)\n", "    opt = optim.Adam(policy.parameters(), lr=1e-3)\n", "    for epoch in range(1, num_epochs+1):\n", "        opt.zero_grad()\n", "        loss = simulate_batch(policy)\n", "        loss.backward()\n", "        nn.utils.clip_grad_norm_(policy.parameters(),1.0)\n", "        opt.step()\n", "        if epoch % 50 == 0:\n", "            print(f\"epoch {epoch}, loss={loss.item():.6f}\")\n", "    return policy\n", "\n", "\n", "# ===== \u30c6\u30b9\u30c8\u30d1\u30b9\u751f\u6210\uff08\u53ef\u8996\u5316\u7528\uff09 =====\n", "def sample_single_path(policy):\n", "    dt = T/J\n", "    sqrt_dt = math.sqrt(dt)\n", "    S = torch.full((1,1), S0, device=device)\n", "    q = torch.full((1,1), q0, device=device)\n", "    Q = torch.zeros((1,1), device=device)\n", "    X = torch.zeros((1,1), device=device)\n", "    eps = torch.randn(1,J,device=device)\n", "    S_ref, V_ref, Q_ref = S0, V, V*T\n", "    q_ref = max(abs(q0),1.0)\n", "    S_hist = torch.zeros(J+1)\n", "    q_hist = torch.zeros(J+1)\n", "    v_hist = torch.zeros(J)\n", "    S_hist[0] = S.item()\n", "    q_hist[0] = q.item()\n", "    policy.eval()\n", "    with torch.no_grad():\n", "        for j in range(J):\n", "            V_t = torch.full((1,1), V, device=device)\n", "            state = torch.cat([\n", "                S/S_ref,\n", "                V_t/V_ref,\n", "                Q/Q_ref,\n", "                q/q_ref\n", "            ], dim=1)\n", "            v = policy(state)\n", "            dq = v * dt\n", "            v_hist[j] = v.item()\n", "            q_next = q - dq\n", "            q_hist[j+1] = q_next.item()\n", "            rho = v / V_t\n", "            L = eta * torch.abs(rho)**(1+phi)\n", "            X = X + (v*S - V_t*L)*dt\n", "            dW = eps[:,j:j+1]*sqrt_dt\n", "            S = S + sigma*dW - k*v*dt\n", "            S_hist[j+1] = S.item()\n", "            Q = Q + V_t*dt\n", "            q = q_next\n", "    t_grid = torch.linspace(0,T,J+1)\n", "    t_mid  = torch.linspace(0,T,J)\n", "    return t_grid, t_mid, S_hist.numpy(), q_hist.numpy(), v_hist.numpy()\n", "\n", "\n", "# ===== \u5b9f\u884c =====\n", "policy = train()\n", "t_grid, t_mid, S_p, q_p, v_p = sample_single_path(policy)\n", "plt.figure(figsize=(10,4))\n", "plt.plot(t_grid, q_p); plt.grid(True); plt.title(\"q_t\")\n", "plt.figure(figsize=(10,4))\n", "plt.plot(t_mid, v_p); plt.grid(True); plt.title(\"v_t\")\n", "plt.show()\n"], "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}