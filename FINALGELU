import math
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

torch.set_default_dtype(torch.float64)
device = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------------------
# モデル & 市場パラメータ
# -----------------------------
S0, q0, V0 = 50.0, 400000.0, 4_000_000.0
sigma_S, eta, phi, k, gamma = 0.45, 0.12, 0.63, 5e-7, 3e-6
mu_V, sigma_V = 0.0, 0.3
lambda_qT = 1e-3

T, J = 1.0, 200
num_epochs, batch_size, hidden_dim = 1000, 256, 128
LR, OPT, WD, M = 1e-2, "Adam", 0.0, 0.9

dt, sqrt_dt = T / J, math.sqrt(T / J)
S_ref, V_ref, Q_ref = S0, V0, V0 * T
q_ref = max(abs(q0), 1.0)

# -----------------------------
# ネットワーク（初期条件の工夫なし）
# -----------------------------
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.v_max = 5 * q0 / T

        self.base = nn.Sequential(
            nn.Linear(4, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
        )
        self.heads = nn.ModuleList([nn.Linear(hidden_dim, 1) for _ in range(J)])
        self.reset_parameters()

    def reset_parameters(self):
        # base: 普通の Xavier 初期化
        for m in self.base:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0.0)

        # head: 全部ゼロ初期化（デフォルト）
        for head in self.heads:
            nn.init.xavier_uniform_(head.weight)
            nn.init.constant_(head.bias, 0.0)

    def forward(self, x, j: int):
        h = self.base(x)
        z = self.heads[j](h)
        v_raw = torch.nn.functional.gelu(z)
        v = torch.clamp(v_raw, -self.v_max, self.v_max)
        return v

# -----------------------------
# Optimizer
# -----------------------------
def make_optim(net):
    if OPT == "Adam":
        return optim.Adam(net.parameters(), lr=LR, weight_decay=WD)
    if OPT == "AdamW":
        return optim.AdamW(net.parameters(), lr=LR, weight_decay=WD)
    return optim.SGD(net.parameters(), lr=LR, momentum=M, weight_decay=WD)

# -----------------------------
# シミュレーション（1バッチ）
# -----------------------------
def simulate_batch(net: Net):
    S = torch.full((batch_size, 1), S0, device=device)
    q = torch.full((batch_size, 1), q0, device=device)
    X = torch.zeros((batch_size, 1), device=device)
    Q = torch.zeros((batch_size, 1), device=device)
    V = torch.full((batch_size, 1), V0, device=device)
    vwN = torch.zeros((batch_size, 1), device=device)
    vwD = torch.zeros((batch_size, 1), device=device)
    eS = torch.randn(batch_size, J, device=device)
    eV = torch.randn(batch_size, J, device=device)

    net.train()
    for j in range(J):
        dWV = eV[:, j:j+1] * sqrt_dt
        V = V * torch.exp((mu_V - 0.5 * sigma_V**2) * dt + sigma_V * dWV)
        V = torch.clamp(V, min=1e-6)

        st = torch.cat([S / S_ref, V / V_ref, Q / Q_ref, q / q_ref], dim=1)
        v = net(st, j)
        dq = v * dt
        q1 = q - dq

        rho = v / V
        L = eta * torch.abs(rho) ** (1 + phi)

        X = X + (v * S - V * L) * dt

        dWS = eS[:, j:j+1] * sqrt_dt
        S = S + sigma_S * dWS - k * v * dt

        Q = Q + V * dt
        vwN = vwN + S * V * dt
        vwD = vwD + V * dt

        q = q1

    VWAP = vwN / vwD
    pnl = X - q0 * VWAP
    pnl_mean = pnl.mean()
    pnl2_mean = (pnl ** 2).mean()
    pnl_var = pnl2_mean - pnl_mean ** 2
    ce_approx = pnl_mean - 0.5 * gamma * pnl_var

    qT = q.squeeze(1)
    loss = -ce_approx + lambda_qT * (qT ** 2).mean()
    return loss

# -----------------------------
# 単一パスのサンプル
# -----------------------------
@torch.no_grad()
def sample_single_path(net: Net):
    S = torch.full((1, 1), S0, device=device)
    q = torch.full((1, 1), q0, device=device)
    Q = torch.zeros((1, 1), device=device)
    V = torch.full((1, 1), V0, device=device)
    eS = torch.randn(1, J, device=device)
    eV = torch.randn(1, J, device=device)

    S_h = torch.zeros(J + 1)
    q_h = torch.zeros(J + 1)
    v_h = torch.zeros(J)
    V_h = torch.zeros(J + 1)

    S_h[0], q_h[0], V_h[0] = S.item(), q.item(), V.item()

    net.eval()
    for j in range(J):
        dWV = eV[:, j:j+1] * sqrt_dt
        V = V * torch.exp((mu_V - 0.5 * sigma_V**2) * dt + sigma_V * dWV)
        V = torch.clamp(V, min=1e-6)
        V_h[j + 1] = V.item()

        st = torch.cat([S / S_ref, V / V_ref, Q / Q_ref, q / q_ref], dim=1)
        v = net(st, j)
        dq = v * dt
        v_h[j] = v.item()
        q1 = q - dq
        q_h[j + 1] = q1.item()

        dWS = eS[:, j:j+1] * sqrt_dt
        S = S + sigma_S * dWS - k * v * dt
        S_h[j + 1] = S.item()

        Q = Q + V * dt
        q = q1

    t = torch.linspace(0, T, J + 1, dtype=torch.float64).cpu().numpy()
    return t, S_h.numpy(), q_h.numpy(), v_h.numpy(), V_h.numpy()

# -----------------------------
# 学習ループ
# -----------------------------
def train_with_snapshots():
    net = Net().to(device)
    opt = make_optim(net)
    q_paths = {}
    loss_hist = []

    for ep in range(1, num_epochs + 1):
        loss = simulate_batch(net)
        loss_value = loss.item()
        loss_hist.append(loss_value)

        opt.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(net.parameters(), 1.0)
        opt.step()

        if ep % 50 == 0:
            print(f"epoch {ep}, loss {loss_value}")
            t, S_h, q_h, v_h, V_h = sample_single_path(net)
            q_paths[ep] = (t, q_h)

    return net, q_paths, loss_hist

# -----------------------------
# 実行
# -----------------------------
net, q_paths, loss_hist = train_with_snapshots()

# Loss
plt.figure(figsize=(6, 4))
plt.plot(loss_hist)
plt.xlabel("epoch")
plt.ylabel("loss")
plt.title("Loss per epoch")
plt.grid(True)
plt.show()

# q_t（薄→濃の赤）
plt.figure(figsize=(6, 4))

epochs = sorted(q_paths.keys())
n = len(epochs)

for i, ep in enumerate(epochs):
    t, q_h = q_paths[ep]
    alpha = 0.2 + 0.8 * (i / (n - 1)) if n > 1 else 1.0
    plt.plot(t, q_h, color="red", alpha=alpha)

plt.title("q(t) snapshots (darker red = later epoch)")
plt.xlabel("t")
plt.ylabel("q(t)")
plt.grid(True)
plt.show()
