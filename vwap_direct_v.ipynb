{"cells": [{"cell_type": "code", "metadata": {}, "source": ["# ================================\n", "# VWAP optimal execution \u00d7 NN \u00d7 q_T=0 soft constraint\n", "# PolicyNet outputs v_t directly (no intermediate u)\n", "# ================================\n", "import math\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "import matplotlib.pyplot as plt\n", "\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "print(\"device:\", device)\n", "\n", "\n", "class PolicyNet(nn.Module):\n", "    def __init__(self, state_dim=4, hidden_dim=128, v_max=None, q0=400000.0, T=1.0):\n", "        super().__init__()\n", "        if v_max is None:\n", "            v_max = 2.0 * q0 / T\n", "        self.v_max = float(v_max)\n", "        self.net = nn.Sequential(\n", "            nn.Linear(state_dim, hidden_dim),\n", "            nn.ReLU(),\n", "            nn.Linear(hidden_dim, hidden_dim),\n", "            nn.ReLU(),\n", "            nn.Linear(hidden_dim, 1),\n", "            nn.Tanh(),\n", "        )\n", "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n", "        return self.v_max * self.net(x)\n", "\n", "\n", "def simulate_batch_signed_volume(\n", "    policy: nn.Module,\n", "    batch_size: int = 256,\n", "    J: int = 200,\n", "    T: float = 1.0,\n", "    S0: float = 50.0,\n", "    q0: float = 400000.0,\n", "    V_const: float = 4_000_000.0,\n", "    sigma: float = 0.45,\n", "    eta: float = 0.12,\n", "    phi: float = 0.63,\n", "    k: float = 5e-7,\n", "    gamma: float = 3e-6,\n", "    lambda_qT: float = 1e-8,\n", "    device: str = \"cpu\",\n", ") -> torch.Tensor:\n", "    policy.train()\n", "    dt = T / J\n", "    sqrt_dt = math.sqrt(dt)\n", "    S = torch.full((batch_size, 1), S0, device=device)\n", "    X = torch.zeros((batch_size, 1), device=device)\n", "    q = torch.full((batch_size, 1), q0, device=device)\n", "    Q = torch.zeros((batch_size, 1), device=device)\n", "    vw_num = torch.zeros((batch_size, 1), device=device)\n", "    vw_den = torch.zeros((batch_size, 1), device=device)\n", "    eps_S = torch.randn(batch_size, J, device=device)\n", "    S_ref = S0\n", "    V_ref = V_const\n", "    Q_ref = V_const * T\n", "    q_ref = max(abs(q0), 1.0)\n", "    for j in range(J):\n", "        V_t = torch.full_like(S, V_const)\n", "        Q = Q + V_t * dt\n", "        S_norm = S / S_ref\n", "        V_norm = V_t / V_ref\n", "        Q_norm = Q / Q_ref\n", "        q_norm = q / q_ref\n", "        state = torch.cat([S_norm, V_norm, Q_norm, q_norm], dim=1)\n", "        v = policy(state)\n", "        dq = v * dt\n", "        q_next = q - dq\n", "        rho = v / V_t\n", "        L_val = eta * torch.abs(rho) ** (1.0 + phi)\n", "        dX = (v * S - V_t * L_val) * dt\n", "        X = X + dX\n", "        dW = eps_S[:, j:j+1] * sqrt_dt\n", "        dS = sigma * dW - k * v * dt\n", "        S = S + dS\n", "        q = q_next\n", "        vw_num = vw_num + S * V_t * dt\n", "        vw_den = vw_den + V_t * dt\n", "    VWAP_T = vw_num / vw_den\n", "    pnl = X - q0 * VWAP_T\n", "    pnl = torch.clamp(pnl, -1e6, 1e6)\n", "    utility = torch.exp(-gamma * pnl)\n", "    penalty = lambda_qT * (q.squeeze(1) ** 2)\n", "    loss = (utility + penalty).mean()\n", "    return loss\n", "\n", "\n", "def train_policy_signed_volume(\n", "    num_epochs: int = 300,\n", "    batch_size: int = 256,\n", "    lr: float = 1e-3,\n", "    lambda_qT: float = 1e-8,\n", "    q0: float = 400000.0,\n", "    T: float = 1.0,\n", "    device: str = \"cpu\",\n", ") -> nn.Module:\n", "    policy = PolicyNet(state_dim=4, hidden_dim=128, v_max=None, q0=q0, T=T).to(device)\n", "    opt = optim.Adam(policy.parameters(), lr=lr)\n", "    for e in range(1, num_epochs + 1):\n", "        opt.zero_grad()\n", "        loss = simulate_batch_signed_volume(\n", "            policy,\n", "            batch_size=batch_size,\n", "            T=T,\n", "            q0=q0,\n", "            lambda_qT=lambda_qT,\n", "            device=device,\n", "        )\n", "        loss.backward()\n", "        nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)\n", "        opt.step()\n", "        if e % 50 == 0:\n", "            print(f\"epoch {e}, loss = {loss.item():.6f}\")\n", "    return policy\n", "\n", "\n", "def sample_single_test_path_signed(\n", "    policy: nn.Module,\n", "    J: int = 200,\n", "    T: float = 1.0,\n", "    S0: float = 50.0,\n", "    q0: float = 400000.0,\n", "    V_const: float = 4_000_000.0,\n", "    sigma: float = 0.45,\n", "    eta: float = 0.12,\n", "    phi: float = 0.63,\n", "    k: float = 5e-7,\n", "    gamma: float = 3e-6,\n", "    device: str = \"cpu\",\n", "):\n", "    policy.eval()\n", "    dt = T / J\n", "    sqrt_dt = math.sqrt(dt)\n", "    S = torch.full((1, 1), S0, device=device)\n", "    X = torch.zeros((1, 1), device=device)\n", "    q = torch.full((1, 1), q0, device=device)\n", "    Q = torch.zeros((1, 1), device=device)\n", "    eps_S = torch.randn(1, J, device=device)\n", "    S_ref = S0\n", "    V_ref = V_const\n", "    Q_ref = V_const * T\n", "    q_ref = max(abs(q0), 1.0)\n", "    S_hist = torch.zeros(J + 1, device=device)\n", "    V_hist = torch.zeros(J,     device=device)\n", "    Q_hist = torch.zeros(J + 1, device=device)\n", "    q_hist = torch.zeros(J + 1, device=device)\n", "    v_hist = torch.zeros(J,     device=device)\n", "    S_hist[0] = S.item()\n", "    Q_hist[0] = Q.item()\n", "    q_hist[0] = q.item()\n", "    with torch.no_grad():\n", "        for j in range(J):\n", "            V_t = torch.full((1, 1), V_const, device=device)\n", "            V_hist[j] = V_t.item()\n", "            Q = Q + V_t * dt\n", "            Q_hist[j + 1] = Q.item()\n", "            S_norm = S / S_ref\n", "            V_norm = V_t / V_ref\n", "            Q_norm = Q / Q_ref\n", "            q_norm = q / q_ref\n", "            state = torch.cat([S_norm, V_norm, Q_norm, q_norm], dim=1)\n", "            v = policy(state)\n", "            dq = v * dt\n", "            v_hist[j] = v.item()\n", "            q_next = q - dq\n", "            q_hist[j + 1] = q_next.item()\n", "            rho = v / V_t\n", "            L_val = eta * torch.abs(rho) ** (1.0 + phi)\n", "            dX = (v * S - V_t * L_val) * dt\n", "            X = X + dX\n", "            dW = eps_S[:, j:j+1] * sqrt_dt\n", "            dS = sigma * dW - k * v * dt\n", "            S = S + dS\n", "            S_hist[j + 1] = S.item()\n", "            q = q_next\n", "    t_grid = torch.linspace(0.0, T, J + 1).cpu().numpy()\n", "    t_mid  = torch.linspace(0.0, T, J).cpu().numpy()\n", "    return t_grid, t_mid, S_hist.cpu().numpy(), V_hist.cpu().numpy(), Q_hist.cpu().numpy(), q_hist.cpu().numpy(), v_hist.cpu().numpy()\n", "\n", "\n", "print(\"Training start...\")\n", "policy = train_policy_signed_volume(\n", "    num_epochs=300,\n", "    batch_size=256,\n", "    lr=1e-3,\n", "    lambda_qT=1e-8,\n", "    q0=400000.0,\n", "    T=1.0,\n", "    device=device,\n", ")\n", "print(\"Training done.\")\n", "\n", "t_grid, t_mid, S_p, V_p, Q_p, q_p, v_p = sample_single_test_path_signed(\n", "    policy,\n", "    J=200,\n", "    T=1.0,\n", "    device=device,\n", ")\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(t_grid, S_p)\n", "plt.xlabel(\"t\"); plt.ylabel(\"S_t\"); plt.title(\"Single path: S_t\"); plt.grid(True)\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(t_grid, q_p)\n", "plt.xlabel(\"t\"); plt.ylabel(\"q_t\"); plt.title(\"Single path: inventory q_t (signed)\"); plt.grid(True)\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(t_mid, v_p)\n", "plt.xlabel(\"t\"); plt.ylabel(\"v_t\"); plt.title(\"Single path: trading speed v_t (signed)\"); plt.grid(True)\n", "\n", "plt.show()\n"], "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}